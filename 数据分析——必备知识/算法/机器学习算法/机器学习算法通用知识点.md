 # 机器学习算法通用知识点
- [机器学习算法通用知识点](#机器学习算法通用知识点)
  - [1. 监督学习、非监督学习、强化学习](#1-监督学习非监督学习强化学习)
    - [1.1. 监督学习](#11-监督学习)
    - [1.2. 无监督学习](#12-无监督学习)
  - [2. 梯度下降](#2-梯度下降)
  - [3. 过拟合和欠拟合](#3-过拟合和欠拟合)
  - [4. 损失函数](#4-损失函数)
  - [5. 正则项](#5-正则项)
    - [L0范数](#l0范数)
    - [L1范数](#l1范数)
    - [L2范数：](#l2范数)
  - [6. 参数与非参数模型](#6-参数与非参数模型)
## 1. 监督学习、非监督学习、强化学习
### 1.1. 监督学习
**监督学习是一种目的明确的训练方式，你知道得到的是什么**
监督并不是指人站在机器旁边看机器做的对不对，而是下面的流程：

1. 选择一个适合目标任务的机器学习模型
2. 把训练集给机器去学习（监督学习需要给数据打标签）
3. 训练得到出方法论
4. 在测试集上使用方法论

**监督学习有2个主要的任务：**
1. 回归：预测连续的、具体的数值。
2. 分类：对各种事物分门别类，用于离散型预测。


### 1.2. 无监督学习
无监督学习本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。它具有三个特点：
1. **无监督学习没有明确目的的训练方式，你无法提前知道结果是什么。**
2. **无监督学习不需要给数据打标签。**
3. **无监督学习几乎无法量化效果如何。**



## 2. 梯度下降
## 3. 过拟合和欠拟合
过拟合：过度学习训练集，在训练集上表现优异，在测试集上表现很差。  
欠拟合：在训练集和测试集上表现都很差
## 4. 损失函数
损失函数就是评价模型所产生的预测结果的一个函数，损失函数的反馈值是机器学习调整参数的重要依据。
如线性回归的损失函数就是最小二乘法。
## 5. 正则项
正则项是在损失函数上增加的偏差因子，L1用于让参数稀疏，L2用于避免过拟合。
### L0范数
L0范数：向量中非0的元素的个数
### L1范数
L1范数：向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）
优点：
1. 特征选择(Feature Selection)
   特征选择的嵌入法就是利用具有L1正则的算法来选择特征。L1范数倾向于产生参数稀疏。例如，模型中有100个系数，但其中只有10个系数是非零系数，也就是说只有这10个变量是有用的，其他90个都是没有用的。  
   **稀疏性**是指矩阵或向量中只有极少个非零系数。
2. 可解释性(Interpretability)
  参数稀疏模型更容易解释

代表算法；Lasso回归
### L2范数：
L2范数：向量各元素的平方和然后求平方根  
让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。
优点：
1. 解决过拟合
   L2范数防止过拟合，提升模型的泛化能力
2. 优化计算
   L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。  
   condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化,也就是系统对微小变化的敏感度。condition number越小越好，最好接近1。

代表算法；岭回归（Ridge Regression）
## 6. 参数与非参数模型
参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；  
非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。