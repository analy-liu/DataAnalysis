 # 机器学习流程（一）----数据预处理
本篇介绍数据预处理流程上的技术需求以及理论知识
- [机器学习流程（一）----数据预处理](#机器学习流程一----数据预处理)
  - [1. 技术需求](#1-技术需求)
  - [2. 方法](#2-方法)
    - [2.1. 结构化与非机构化数据](#21-结构化与非机构化数据)
    - [2.2. 数据清洗(data cleaning)](#22-数据清洗data-cleaning)
      - [2.2.1. 分析数据](#221-分析数据)
      - [2.2.2. 去除唯一属性](#222-去除唯一属性)
      - [2.2.3. 缺失值处理](#223-缺失值处理)
      - [2.2.4. 重复值处理](#224-重复值处理)
      - [2.2.5. 异常值处理](#225-异常值处理)
      - [2.2.6. 噪音处理](#226-噪音处理)
      - [2.2.7. 数据转换(数据标准化)](#227-数据转换数据标准化)
      - [2.2.8. 处理类别不平衡问题](#228-处理类别不平衡问题)
    - [2.3. 特征工程(Feature Engineering)](#23-特征工程feature-engineering)
      - [2.3.1. 数据降维（Dimensionality Reduction）-PAC](#231-数据降维dimensionality-reduction-pac)
      - [2.3.2. 特征选取（Feature Selection）](#232-特征选取feature-selection)
      - [2.3.3. 特征提取（Feature Extraction）](#233-特征提取feature-extraction)
  - [3. 参考文献](#3-参考文献)
## 1. 技术需求
1. python
   1. pandas：
   2. numpy：
   3. 数据可视化：matplotlib.pyplot、pyecharts：
2. excel
## 2. 方法
数据预处理可以称为建立机器学习模型最重要的一步，数据预处理没做好，建立的模型就没有意义或者无效。
### 2.1. 结构化与非机构化数据
数据大致分为非结构化数据和结构化数据。 
结构化数据是以特定格式提供的信息，因此易于阅读。NoSQL 数据库中的表，变量作为列，记录作为行或者键值对就是一个结构化数据的例子。   
非结构化数据是大部分为格式自由、数据类型不一致的数据。医生的手写处方和从博客中收集的影评是两个非结构化数据的例子。  

1. 结构化数据处理
   1. 数据清洗：去除唯一属性、缺失值、重复值、异常值处理、数据标准化等
   2. 特征工程：特征选择、特征编码、降维、增维等
2. 非结构化数据处理
   1. Web页面信息内容提取；
   2. 结构化处理（含文文本的词汇切分、词性分析、歧义处理等）；
   3. 语义处理（含实体提取、词汇相关度、句子相关度、篇章相关度、句法分析等）
   4. 文本建模（含向量空间模型、主题模型等）
   5. 隐私保护（含社交网络的连接型数据处理、位置轨迹型数据处理等）
### 2.2. 数据清洗(data cleaning)
数据清洗就是对原始数据通过丢弃、填充、替换、去重等操作，实现去除异常、纠正错误、补足缺失的目的。  
在数据清洗过程中，首先需要分析数据，通过分析结果处理缺失值、异常值和重复值。最后处理类别不平衡问题。
#### 2.2.1. 分析数据
对数据进行描述性统计分析，知道数据基本情况，也可通过作图方式了解数据质量，有无异常（离群）点，有无噪音等。
python的pandas库中的主要统计特征函数如下：
|方法名|函数功能|
|:-:|--|
|sum()|计算数据样本的总和(按列计算)|
|mean()|计算数据样本的算术平均数|
|var()|计算数据样本的方差|
|std()|计算数据样本的标准差|
|corr()|计算数据样本的Spearman(Pearson)相关矩阵|
|cov()|计算数据样本的协方差矩阵|
|skew()|样本值的偏度|
|kurt()|样本值的峰度|
|describe()|给出样本的基本描述|
#### 2.2.2. 去除唯一属性
唯一属性通常是一些id属性，这些属性并不能刻画样本自身的分布规律，所以简单地删除这些属性即可。 
#### 2.2.3. 缺失值处理
缺失值的处理方法：
1. 删除含有缺失值的特征
   直接删除带有缺失值的数据
   1. 删除行记录（**整行删除**）
   适合缺失值数量较小，并且是随机出现的，删除它们对整体数据影响不大的情况。
   2. 删除列字段（**整列删除**）
   缺失值如果占了95%以上，可以直接去掉这个维度的数据了。
2. 补全缺失值
   1. 统计法
    对于数值型的数据，正态分布使用均值、偏态数据使用中位数等方法补足；
    对于分类型数据，使用类别众数最多的值补足。
   2. 插补法
   随机法：从总体中随机抽取某个样本代替缺失样本；
   最近法：寻找与该样本最接近的样本，使用其该属性数值来补全。
   3. 高维映射
   将属性映射到高维空间，采用**独热码编码**（**one-hot**）技术。将包含 K 个离散取值范围的属性值扩展为 K+1 个属性值，若该属性值缺失，则扩展后的第 K+1 个属性值置为 1。
   优点：精确，保留了所有的信息，也未添加任何额外信息
   缺点：大大增加数据的维度，计算量大大提升，且只有在样本量非常大的时候效果才好
   4. 模型法
   可以用回归、使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。例如，利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。
3. 不处理
   很多模型对于缺失值有容忍度或灵活的处理方法。
   常见的能够自动处理缺失值的模型包括：KNN、决策树和随机森林、神经网络和朴素贝叶斯等。

总结：对于缺失值的处理思路是先通过一定方法**找到缺失值**，接着分析缺失值在整体样本中的**分布**占比以及缺失值是否具有显著的无规律分布特征，然后考虑后续要使用的**模型中是否能满足缺失值的自动处理**，最后决定采用哪种缺失值处理方法。在选择处理方法时，注意投入的时间、精力和产出价值，毕竟，处理缺失值只是整个数据工作的冰山一角而已。
#### 2.2.4. 重复值处理
主要方法：对数据集直接去重
```python
df.drop_duplicates()
"会返回一个返回一个移除了重复行的DataFrame"
```
去重时需要考虑重复值是否反应了业务问题，与样本不均衡问题。
#### 2.2.5. 异常值处理
判断异常值方法：
1. 画图法
   优点：直观
   缺点：数据量大时速度慢
2. 标准差
   如果数据服从正态分布，在3σ原则下，异常值为一组测定值中与平均值的偏差超过3倍标准差的值。
   $$
   P(\mu-1σ\le X \le \mu+1σ)\approx0.682\\[2ex]
   P(\mu-2σ\le X \le \mu+2σ)\approx0.954\\[2ex]
   P(\mu-3σ\le X \le \mu+3σ)\approx0.997
   $$

3. 箱型图
   
   QL为下四分位数，QU为上四分位数，IQR=QU-QL
   **当一个数值大于QU+1.5IQR或者小于QL-1.5IQR时，被称为异常值**
   四分位数具有鲁棒性，异常值不会对四分位数产生影响。因此箱型图识别异常值比较客观，在识别异常值时有一定的优越性。
4. 模型法
5. 聚类法

处理异常值的方法：
1. 删除异常值----明显看出是异常且数量较少可以直接删除
2. 不处理---如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的。
3. 平均值替代----损失信息小，简单高效。
4. 视为缺失值----可以按照处理缺失值的方法来处理

#### 2.2.6. 噪音处理
噪音通常是数据集中的错误值。
处理方法：
1. **分箱法**

分箱方法通过考察数据的“近邻”（即，周围的值）来光滑有序数据值。这些有序的值被分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此它进行局部光滑。

- 用箱均值光滑：箱中每一个值被箱中的平均值替换。
- 用箱中位数平滑：箱中的每一个值被箱中的中位数替换。
- 用箱边界平滑：箱中的最大和最小值同样被视为边界。箱中的每一个值被最近的边界值替换。

一般而言，宽度越大，光滑效果越明显。箱也可以是等宽的，其中每个箱值的区间范围是个常量。分箱也可以作为一种离散化技术使用.

2. **回归法**

可以用一个函数拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”直线，使得一个属性能够预测另一个。多线性回归是线性回归的扩展，它涉及多于两个属性，并且数据拟合到一个多维面。使用回归，找出适合数据的数学方程式，能够帮助消除噪声。
#### 2.2.7. 数据转换(数据标准化)
数据标准化是将样本的属性缩放到某个指定的范围。

**数据标准化的原因**：
某些算法要求样本具有零均值和单位方差；
需要消除样本不同属性具有不同量级时的影响：①数量级的差异将导致量级较大的属性占据主导地位；②数量级的差异将导致迭代收敛速度减慢；③依赖于样本距离的算法对于数据的数量级非常敏感。
归一化后求优过程范围变小，寻优过程变得平缓，更容易正确收敛到最优解

**对测试集的缩放**：
我们在对测试集做特征缩放时要使用先前对训练集进行特征缩放时所使用的参数$\mu,\sigma$，而不能根据测试集另算一组均值方差做特征缩放。 原则：避免测试数据信息引入训练过程。 

**哪些机器学习算法不需要(需要)做归一化?**
概率模型（树形模型）不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、SVM、LR、Knn、KMeans之类的最优化问题就需要归一化。

数据标准化的方法：
1. min-max标准化（归一化）
   把最大值归为1，最小值归为0
   $$
   x' = \frac{x-x_{min}}{x_{max}-x_{min}}
   $$
   python中MinMaxScaler方法可以实现
   from sklearn.preprocessing import MinMaxScaler
2. z-score标准化（规范化）
   将数据均值化为0，方差化为1
   $$
   x' = \frac{x-\mu}{\sigma}
   $$
   python中StandardScaler方法可以实现
   from sklearn.preprocessing import StandardScaler
3. 正则化
   数据正则化是将样本的某个范数（如L1范数）缩放到到位1，正则化的过程是针对单个样本的，对于每个样本将样本缩放到单位范数。

#### 2.2.8. 处理类别不平衡问题

类别不平衡：是指分类任务中存在某个或者某些类别的样本数量远多于其他类别的样本数量的情况。

1. 对大类数据采取欠采样
2. 对小类数据采取过采样

### 2.3. 特征工程(Feature Engineering)
特征工程就是**对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用**。
#### 2.3.1. 数据降维（Dimensionality Reduction）-PAC
在数据处理中，经常会遇到特征维度比样本数量多得多的情况，如果拿到实际工程中去跑，效果不一定好。一是因为冗余的特征会带来一些噪音，影响计算的结果；二是因为无关的特征会加大计算量，耗费时间和资源。所以我们通常会对数据重新变换一下，再跑模型。数据变换的目的不仅仅是降维，还可以消除特征之间的相关性，并发现一些潜在的特征变量。
1. PAC的目的
   PCA是一种在尽可能减少信息损失的情况下找到某种方式降低数据的维度的方法。  
   注意：
   如果变量之间的方差很大，或者变量的量纲不统一，我们必须先标准化再进行主成分分析。
   主成分分析通常会得到协方差矩阵和相关矩阵。这些矩阵可以通过原始数据计算出来。协方差矩阵包含平方和与向量积的和。相关矩阵与协方差矩阵类似，但是第一个变量，也就是第一列，是标准化后的数据。
2. PAC优缺点
   优点：
   1. 以方差衡量信息的无监督学习，不受样本标签限制。
   2. 各主成分之间正交，可消除原始数据成分间的相互影响
   3. 可减少指标选择的工作量

   缺点：
   1. 主成分解释其含义往往具有一定的模糊性，不如原始样本完整
   2. 贡献率小的主成分往往可能含有对样本差异的重要信息
   3. 特征值矩阵的正交向量空间是否唯一有待讨论
3. PAC的过程


#### 2.3.2. 特征选取（Feature Selection）
特征选择，就是从多个特征中，挑选出一些对结果预测最有用的特征。因为原始的特征中可能会有冗余和噪声。  

特征选择和降维的区别：
前者只踢掉原本特征里和结果预测关系不大的， 后者做特征的计算组合构成新特征。  

**与业务相关的特征选择方法**
特征选择的第一步是找懂业务的专家，让他们给一些建议，向他们咨询哪些因素（特征）会对目标有影响，较大影响的和较小影响的都要。这些特征就是我们的特征的第一候选集。  

**与业务无关的特征选择方法**
1. 过滤法

方差； 
定一个阈值，方差小于阈值的特征舍弃掉

相关系数：  
设定一个阈值，选择相关系数较大的部分特征  

假设检验：  
比如卡方检验。卡方检验可以检验某个特征分布和输出值分布之间的相关性。  
在sklearn中，可以使用chi2这个类来做卡方检验得到所有特征的卡方值与显著性水平P临界值，我们可以给定卡方值阈值， 选择卡方值较大的部分特征。

互信息：  
即从信息熵的角度分析各个特征和输出值之间的关系评分。互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留。  
在sklearn中，可以使用mutual_info_classif(分类)和mutual_info_regression(回归)来计算各个输入特征和输出值之间的互信息。

2. 包装法

最常用的包装法是递归消除特征法(recursive feature elimination,以下简称RFE)。递归消除特征法使用一个机器学习模型来进行多轮训练，每轮训练后，消除贡献小的特征，再基于新特征集进行下一轮训练。  

应用在逻辑回归的过程：用全量特征跑一个模型；根据线性模型的系数(体现相关性)，删掉5-10%的弱特征，观察准确率/auc的变化；逐步进行， 直至准确率/auc出现大的下滑停止。

3. 嵌入法

最常用的是使用L1正则化和L2正则化来选择特征。  

正则化惩罚项越大，那么模型的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0. 但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。  

一般可以得到特征系数coef或者特征重要度(feature importances)的算法才可以做为嵌入法的基学习器

4. 遗传算法

遗传算法常用于**监督式特征提取**与最优化，比如去找神经网络的最佳权重。  

优点：
1. 在穷举搜索不可行的情况下，对高维数据集使用遗传算法会相当有效。
2. 当用到的算法需要预处理数据却没有内置的特征选取机制(如最近邻分类算法），而你又必须**保留最原始的特征**(也就是不能用任何主成分分析算法)，遗传算法就成了你最好的选择。这一情况在要求透明、可解释方案的商业环境下时有发生。  

缺点：实施复杂、不简洁
#### 2.3.3. 特征提取（Feature Extraction）
根据业务知识进行特征提取的常用方法：
1. 若干项特征加和
2. 若干项特征之差
3. 若干项特征乘积
4. 若干项特征除商
## 3. 参考文献
特征工程之数据预处理——鑫鑫淼淼焱焱 https://zhuanlan.zhihu.com/p/56479125  
机器学习基础与实践（一）----数据清洗 - Charlotte77 - 博客园 https://www.cnblogs.com/charlotte77/p/5606926.html  
机器学习基础与实践（三）----数据降维之PCA - Charlotte77 - 博客园 https://www.cnblogs.com/charlotte77/p/5625984.html  
特征工程之特征选择-刘建平Pinard https://www.cnblogs.com/pinard/p/9032759.html  
