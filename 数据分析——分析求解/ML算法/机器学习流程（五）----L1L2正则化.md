# L1L2正则化（regularization）

正则化，就是规范化的意思，机器学习中通常会在损失函数中加入正则项，目的是为了防止过拟合，公式很简单

$$Loss = Error(y,\hat y)+\lambda \phi(w)$$

$\lambda \phi(w)$为正则项，$\lambda$为正则项系数，越大，正则化的约束越强  
可以从公式中看到，想要损失函数最小，不仅要Error最小，还要w最小。

正则又分L1、L2范式，也可同时使用，称为弹性网络(Elastic Net)

L1正则项：$\lambda \sum^N_{i=1}|w_i|$
L2正则项：$\lambda \sum^N_{i=1}w_i^2$
弹性网络：$\lambda \rho \sum^N_{i=1}|w_i|+\frac{\lambda(1-\rho)}{2}\sum^N_{i=1}w_i^2$

## 从概率统计角度  

无正则化就是在求MLE（最大似然估计）  
$$MLE=argmaxp(X|w)$$
有正则化就是在求MAP（最大后验估计）,根据贝叶斯公式可表示为公式（2）  
$$
\begin{align}
MAP&=argmaxp(w|X)\\
&=argmax \frac{p(X|w)p(w)}{p(X)}\\
&=argmin[-logp(w|X)]\\
&=argmin[-logp(X|w)-logp(w)+\overbrace{logp(X)}^{常数}]\\
&=argmin[-logp(X|w)-\overbrace{logp(w)}^{正则项}]\\
\end{align}
$$

L1正则就是先验是拉普拉斯分布的情况，在一通推导后，相当于增加了绝对值约束  
$$
\begin{align}
logp(w)&=log\prod^m_{j=1} \frac{1}{2\lambda}e^{-\frac{|w_j|}{\lambda}}\\
&=\sum^m_{j=1}-\frac{|w_j|}{\lambda}\overbrace{log\frac{1}{2\lambda}}^{常数}\\
argmaxlog p(w)&=argmax\sum^m_{j=1}-|w_j|
\end{align}\\
$$

L2正则就是先验是高斯分布的情况，在一通推导后，相当于增加了平方约束  
$$
\begin{align}
logp(w)&=log\prod^m_{j=1} \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(w_j-u)^2}{2\sigma^2}}\\
if u=0:&=\sum^m_{j=1}-\frac{w_j^2}{2\sigma^2}\overbrace{log\frac{1}{\sqrt{2\pi}\sigma}}^{常数}\\
argmaxlog p(w)&=argmax\sum^m_{j=1}-\frac{w_j^2}{2}
\end{align}\\
$$

## L1L2正则化区别

![正则化图解](https://raw.githubusercontent.com/analy-liu/PersonalImgaes/main/images/正则化图解.png)
  
左边是L2正则，右边是L1正则。蓝色圈表示没有限制的损失函数随着w迭代寻找着最小化的过程的损失等高线，同一个圈上损失值相同。**蓝色圈和橙色圈之和就是目标函数值，目标函数最小化的点往往出现在蓝圈和橙圈相交的点**

可以看到L2正则处处可导，方便计算  
而L1正则存在拐点不是处处可微，但L1正则化的最优参数值恰好是$w_1=0$的时候，实现了模型的稀疏化，也就是将部分不重要的特征权重设置为0