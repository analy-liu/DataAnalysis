 # 机器学习流程（三）----模型评价指标
- [机器学习流程（三）----模型评价指标](#机器学习流程三----模型评价指标)
  - [1. 回归模型评价指标](#1-回归模型评价指标)
    - [1.1. MSE、RMSE与MAE](#11-msermse与mae)
    - [1.2. R-square与Adjusted R-Square](#12-r-square与adjusted-r-square)
    - [1.3. 在python库中调用函数](#13-在python库中调用函数)
  - [2. 分类模型评价指标](#2-分类模型评价指标)
    - [2.1. 精准率(precision)、召回率(recall)和f1-score](#21-精准率precision召回率recall和f1-score)
    - [2.2. 准确率(accuracy)和错误率(errorrate)](#22-准确率accuracy和错误率errorrate)
    - [2.3. ROC曲线、AUC](#23-roc曲线auc)
    - [2.4. 对数损失(logloss)](#24-对数损失logloss)
    - [2.5. 在python库中调用函数](#25-在python库中调用函数)
  - [3. 参考文献](#3-参考文献)

## 1. 回归模型评价指标
先介绍一下回归的一些知识，方便理解
![回归图解](https://raw.githubusercontent.com/analy-liu/PersonalImgaes/main/images/回归图解.png)
$$
SSE(残差平方和) = \sum^n_{i=1} (y_i-\hat y_i)^2 \\[2ex]
SSR(回归平方和) = \sum^n_{i=1} (\hat y_i-\bar y)^2\\[2ex]
SST(总平方和) = SSE + SSR = \sum^n_{i=1} (y_i-\bar y)^2\\[2ex]
df(自由度)\\[2ex]
$$

### 1.1. MSE、RMSE与MAE
1. Mean Squared Error, 均方残差
   $$MSE = \frac{SSE}{df}$$
2. Root Mean Squared Error，均方根误差
   $$RMSE(估计标准误差) = \sqrt{MSE}$$
3. Mean Absolute Error，平均绝对误差
   $$MAE = \frac{1}{df}\sum^n_{i=1} |y_i-\hat y_i|$$

### 1.2. R-square与Adjusted R-Square
1. R-square(决定系数)
   $$R^2 = 1-\frac{SSE}{SST}$$
2. Adjusted R-Square (校正决定系数）
   $$R^2_a = 1-(1-R^2)\frac{n-1}{n-k-1}$$
   其中n为数据量，k为特征量
### 1.3. 在python库中调用函数
```python
from sklearn import metrics
MSE = metrics.mean_squared_error(y_true, y_pred)
RMSE = np.sqrt(MSE)
MAE = metrics.mean_absolute_error(y_true, y_pred)
r2 = metrics.r2_score(y_true, y_pred)
r2_Adjusted = 1-(1-r2)*(n-1)/(n-k-1)
# 皮尔森相关系数Pearson Correlation
from scipy import stats
r_pearson = stats.pearsonr(y_true, y_pred)[0]
```
## 2. 分类模型评价指标
真正例（True Positive，TP）：真实类别为正例，预测类别为正例。
假正例（False Positive，FP）：真实类别为负例，预测类别为正例。
假负例（False Negative，FN）：真实类别为正例，预测类别为负例。
真负例（True Negative，TN）：真实类别为负例，预测类别为负例。
||实际真|实际假|
|--|--|--|
|预测真|TP|FP|
|预测假|FN|TN|
### 2.1. 精准率(precision)、召回率(recall)和f1-score
**1. precision与recall**
precision与recall只可用于二分类问题
$$
精准率(precision) = \frac{TP}{TP+FP}\\[2ex]
召回率(recall) = \frac{TP}{TP+FN}
$$
precision是指模型预测为真时预测对的概率，即模型预测出了100个真，但实际上只有90个真是对的，precision就是90%  
recall是指模型预测为真时对实际正确的覆盖率，即测试集里实际有100个真，你预测到了其中的80个，recall就是80%

**2. f1-score**
f1-score是精确率(P)和召回率(R)的调和均值
$$
\frac{2}{F_1} = \frac{1}{P}+\frac{1}{R} = \frac{2TP+FP+FN}{TP}\\[2ex]
F1 = \frac{2PR}{P+R} = \frac{2TP}{2TP+FP+FN}
$$
还可以调节精确率(P)和召回率(R)权重
$$
F_\alpha = \frac{(1+\alpha^2)PR}{\alpha^2P+R}
$$

有两种情形：
1. 宁可错杀一千，不可放过一个
   这种时候需要牺牲精准率，提高召回率，不能放过一个坏人，预测为坏人时的正确率很低，也要覆盖到坏人。
2. 宁可放过一千，不可错杀一个
   这种时候需要牺牲召回率，提高精准率。不能错怪一个好人，预测为坏人时的覆盖率很低，也不能把好人当成坏人。
### 2.2. 准确率(accuracy)和错误率(errorrate)
accuracy和errorrate可用于多分类问题
$$
准确率(accuracy) = \frac{TP+TN}{TP+FP+FN+TN}\\[2ex]
错误率(errorrate) = \frac{FP+FN}{TP+FP+FN+TN}\\[2ex]
accuracy + errorrate = 1
$$
准确率(accuracy)即判断准确的概率

### 2.3. ROC曲线、AUC
ROC曲线、AUC这两个指标主要作为不平衡分类的评价指标
**1. ROC**（接受者操作特征Receiver Operating Characteristic)  
ROC曲线纵坐标是真正率，横坐标是假正率。

$$
真正率(True\ Positive\ Rate) = \frac{TP}{TP+FN}\\[2ex]
假正率(False\ Positive\ Rate) = \frac{FP}{FP+TN}
$$
真正率=召回率，真的被认为是真的概率  
假正率=1-真正率，假的被误认为真的概率  
TPR=1,FPR=1的点对应的模型为把每个实例都预测为正类。TPR=0,FPR=0的点对应的模型为把每个实例都预测为负类。TPR=1,FPR=0的点对应的模型为理想模型
![20210311000328](https://raw.githubusercontent.com/analy-liu/PersonalImgaes/main/images/20210311000328.png)

**2. AUC**(Area under curve)
AUC是ROC曲线下面积。  
AUC是指**随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值比分类器输出该负样本为正的那个概率值要大的可能性**。  
AUC越接近1，说明分类效果越好  
AUC=0.5，说明模型完全没有分类效果  
AUC<0.5，则可能是标签标注错误等情况造成

**举例计算**：
|ID|label|predict|
|--|--|--|
|A|0|0.5|
|B|0|0.4|
|C|1|0.4|
|D|1|0.7|

这里有2个正例，2个负例，两两结合一共有四个正负样本对，即：  
\[[A,C],[A,D],[B,C],[B,D]]
对应的正例概率大于负例概率的概率为(正负概率相同记0.5)：
\[0, 1, 0.5, 1]
$$AUC = \frac{0+1+0.5+1}{4} = 0.625$$
**AUC公式计算**：
$$AUC = \frac{\sum_{i\in P}rank_i-\frac{P(P+1)}{2}}{P\times N}$$
其中$\sum_{i\in P}rank_i$表示在将predict概率增序排序后，对正例的排序号进行求和。P代表正例数，N代表负例数。
还是用上面的例子计算：
|ID|label|predict|rank|Adjusted-rank|
|--|--|--|--|--|
|B|0|0.4|1|1.5|
|C|1|0.4|2|1.5|
|A|0|0.5|3|3|
|D|1|0.7|4|4|

其中因为B、C的predict相同，所以Adjusted-rank为
$\frac{1+2}{2}=1.5$  
$$AUC = \frac{(1.5+4)-\frac{2(2+1)}{2}}{2\times2} = 0.625$$
### 2.4. 对数损失(logloss)
对数损失(logistic loss，logloss)是对预测概率的似然估计  
logloss衡量的是预测概率分布和真实概率分布的差异性，取值越小越好。与AUC不同，logloss对预测概率敏感。  
logloss反映了样本的平均偏差，经常作为模型的损失函数来做优化。  
$$LogLoss = -\frac{1}{n}\sum_i[y_ilog(y'_i)+(1-y_i)log(1-y'_i)]$$
其中$n$为样本数量，对于第$i$个样本，它的真实label为$y_i\in{0,1}$，预测概率为$y'_i\in[0,1]$
### 2.5. 在python库中调用函数
```python
from sklearn import metrics

recall = metrics.recall_score(y_true, y_pred)
precision = metrics.precision_score(y_true, y_pred)
f1 = metrics.f1_score(y_true, y_pred)
accuracy = metrics.accuracy_score(y_true, y_pred)
auc = metrics.roc_auc_score(y_true, y_score)
```
## 3. 参考文献
杜博亚, https://zhuanlan.zhihu.com/p/43405406  
kingsam_, https://blog.csdn.net/qq_22238533/article/details/78666436