 # 机器学习流程（总论）----数据分析全生命周期对应技术与方法
- [机器学习流程（总论）----数据分析全生命周期对应技术与方法](#机器学习流程总论----数据分析全生命周期对应技术与方法)
  - [1. 数据获取](#1-数据获取)
    - [1.1. 技术需求](#11-技术需求)
    - [1.2. 方法](#12-方法)
      - [1.2.1. 内部来源](#121-内部来源)
      - [1.2.2. 外部来源](#122-外部来源)
      - [1.2.3. 埋点](#123-埋点)
  - [2. 数据预处理](#2-数据预处理)
    - [2.1. 技术需求](#21-技术需求)
    - [2.2. 方法](#22-方法)
      - [2.2.1. 结构化与非机构化数据](#221-结构化与非机构化数据)
      - [2.2.2. 数据清洗(data cleaning)](#222-数据清洗data-cleaning)
      - [2.2.3. 特征工程](#223-特征工程)
  - [3. 可视化](#3-可视化)
    - [3.1. 技术需求](#31-技术需求)
  - [4. 分析与建模](#4-分析与建模)
    - [4.1. 技术需求](#41-技术需求)
    - [4.2. 机器学习算法](#42-机器学习算法)
      - [4.2.1. 监督学习](#421-监督学习)
        - [4.2.1.1. 分类+回归](#4211-分类回归)
        - [4.2.1.2. 回归](#4212-回归)
        - [4.2.1.3. 分类](#4213-分类)
      - [4.2.2. 非监督学习](#422-非监督学习)
        - [4.2.2.1. 聚类](#4221-聚类)
        - [4.2.2.2. 降维](#4222-降维)
      - [4.2.3. 强化学习](#423-强化学习)
    - [4.3. 模型评价指标](#43-模型评价指标)
  - [5. 参考文献](#5-参考文献)

数据分析整个流程大致如下：
1. 分析需求：
   分析项目的目的和需求，了解这个项目属于什么问题，要达到什么效果。
2. 数据获取：
   数据一般有几种获取方式：数据库提取、爬虫收集、提前埋点
3. 数据清洗
   获取数据之后，做基本的数据清洗。
4. 特征工程：
   这个需要耗费很大的精力，如果特征工程做的好，那么，后面选择什么算法其实差异不大，反之，不管选择什么算法，效果都不会有突破性的提高。
5. 选择算法：
   可以把所有能跑的算法先跑一遍，看看效果，分析模型评价指标
   模型评价指标:
   1. 精准率(precesion)与召回率(recall)
   2. 准确率(accuracy)和错误率(errorrate)
   3. f1-score
   4. ROC曲线、AUC
   看看有没有什么异常（比如有好几个算法precision特别好，但是recall特别低，这就要从数据中找原因，或者从算法中看是不是因为算法不适合这个数据），如果没有异常，那么就进行下一步，
6. 算法调优：
   选择一两个跑的结果最好的算法进行调优。调优的方法很多，调整参数的话可以用网格搜索、随机搜索等，调整性能的话，可以根据具体的数据和场景进行具体分析。
   调优后再跑一边算法，看结果有没有提高，如果没有，找原因，
      1. 特征问题就回到第四步再进行特征工程；
      2. 数据质量问题就回到第三步看数据清洗有没有遗漏，异常值是否影响了算法的结果；
      3. 算法问题就回到第五步，看算法流程中哪一步出了问题。
   如果实在不行，可以搜一下相关的论文，看看论文中有没有解决方法。这样反复来几遍，就可以出结果了，
7. 最终结果：
   写技术文档和分析报告，再向业务人员或产品讲解我们做的东西，然后他们再提建议/改需求，不断循环，最后代码上线，改bug，直到结项。
以下是具体步骤的技术需求和方法
## 1. 数据获取
### 1.1. 技术需求
   1. SQL
   2. excel
   3. 爬虫:python(requests、lxml、selenium等包)

### 1.2. 方法
   在数据分析之前首先需要进行数据获取，首先要界定分析范围和数据来源，保证分析过程合理有效。数据分析可分为内部来源和外部来源
#### 1.2.1. 内部来源
   1. 企业内部数据库
   2. 机器、传感器数据
   3. 问卷调查
   4. 埋点
#### 1.2.2. 外部来源
   1. 互联网公开信息
   2. 付费数据
   3. 网络采集

#### 1.2.3. 埋点
是在应用中特定的流程收集一些信息，用来跟踪应用使用的状况，后续用来进一步优化产品或是提供运营的数据支撑
2. 埋点方法
   1. 自己公司研发在产品中注入代码统计，并搭建起相应的后台查询。
   2. 第三方统计工具，如友盟、神策、Talkingdata、GrowingIO等。
3. 常用埋点
   1. 访问次数（Visits）与访问人数（Vistors）
   2. 停留时长
   3. 跳出率：进入网站后没有其他操作就退出
   4. 退出率：进入网站后还浏览了其他内容后退出
   5. 转化率

## 2. 数据预处理
### 2.1. 技术需求
1. python(pandas、numpy、matplotlib.pyplot等包)
2. excel
### 2.2. 方法
数据预处理可以称为建立机器学习模型最重要的一步，数据预处理没做好，建立的模型就没有意义或者无效。
#### 2.2.1. 结构化与非机构化数据
数据大致分为非结构化数据和结构化数据。 
结构化数据是以特定格式提供的信息，因此易于阅读。NoSQL 数据库中的表，变量作为列，记录作为行或者键值对就是一个结构化数据的例子。   
非结构化数据是大部分为格式自由、数据类型不一致的数据。医生的手写处方和从博客中收集的影评是两个非结构化数据的例子。  

1. 结构化数据处理
   1. 数据清洗：去除唯一属性、缺失值、重复值、异常值处理、数据标准化等
   2. 特征工程：特征选择、特征编码、降维、增维等
2. 非结构化数据处理
   1. Web页面信息内容提取；
   2. 结构化处理（含文文本的词汇切分、词性分析、歧义处理等）；
   3. 语义处理（含实体提取、词汇相关度、句子相关度、篇章相关度、句法分析等）
   4. 文本建模（含向量空间模型、主题模型等）
   5. 隐私保护（含社交网络的连接型数据处理、位置轨迹型数据处理等）
#### 2.2.2. 数据清洗(data cleaning)
数据清洗就是对原始数据通过丢弃、填充、替换、去重等操作，实现去除异常、纠正错误、补足缺失的目的。  
在数据清洗过程中，主要处理的是缺失值、异常值和重复值。
#### 2.2.3. 特征工程
特征工程就是**对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用**。

## 3. 可视化
### 3.1. 技术需求
1. python
   1. 基础：matplotlib
   2. 统计：Seaborn
   3. 交互：Pyecharts、Bokeh、Plotly
   4. 地图：Mapbox、Geoplotlib
2. Tableau
3. Power BI
4. excel数据透视图

## 4. 分析与建模
### 4.1. 技术需求
1. python(sklearn、sqlalchemy等包)
2. tensorflow(或pytorch)等深度学习框架
3. 基础的机器学习算法
4. 深度学习算法
5. NLP(Natural language processing)
### 4.2. 机器学习算法
#### 4.2.1. 监督学习
监督学习是一种目的明确的训练方式，你知道得到的是什么

监督并不是指人站在机器旁边看机器做的对不对，而是下面的流程：

1. 选择一个适合目标任务的机器学习模型
2. 把训练集给机器去学习（监督学习需要给数据打标签）
3. 训练得到出方法论
4. 在测试集上使用方法论

**监督学习有2个主要的任务：**
1. 回归：预测连续的、具体的数值。
2. 分类：对各种事物分门别类，用于离散型预测。

##### 4.2.1.1. 分类+回归
|算法|类型|简介|
|--|--|--|
|K邻近|	分类+回归|	通过搜索K个最相似的实例（邻居）的整个训练集并总结那些K个实例的输出变量，对新数据点进行预测。|
|Adaboosting|	分类+回归|	Adaboost目的就是从训练数据中学习一系列的弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。|
|神经网络|	分类+回归|	它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。|
##### 4.2.1.2. 回归
|算法|类型|简介|
|--|--|--|
|线性回归|	回归|	线性回归是处理回归任务最常用的算法之一。该算法的形式十分简单，它期望使用一个超平面拟合数据集（只有两个变量的时候就是一条直线）。|
|回归树|	回归|	回归树（决策树的一种）通过将数据集重复分割为不同的分支而实现分层学习，分割的标准是最大化每一次分离的信息增益。这种分支结构让回归树很自然地学习到非线性关系。|


##### 4.2.1.3. 分类
|算法|类型|简介|
|--|--|--|
|朴素贝叶斯|	分类|	贝叶斯分类法是基于贝叶斯定定理的统计学分类方法。它通过预测一个给定的元组属于一个特定类的概率，来进行分类。朴素贝叶斯分类法假定一个属性值在给定类的影响独立于其他属性的 —— 类条件独立性。|
|决策树|	分类|	决策树是一种简单但广泛使用的分类器，它通过训练数据构建决策树，对未知的数据进行分类。|
|SVM|	分类|	支持向量机把分类问题转化为寻找分类平面的问题，并通过最大化分类边界点距离分类平面的距离来实现分类。|
|逻辑回归|	分类|	逻辑回归是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。|
|线性判别分析算法（LDA）|分类|处理多分类问题|
#### 4.2.2. 非监督学习
无监督学习本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。它具有三个特点：
1. **无监督学习没有明确目的的训练方式，你无法提前知道结果是什么。**
2. **无监督学习不需要给数据打标签。**
3. **无监督学习几乎无法量化效果如何。**
##### 4.2.2.1. 聚类
|类别|算法|简介|方法|
|--|--|--|--|
|聚类|K均值聚类|制定分组的数量为K，自动进行分组|1. 定义 K 个重心。一开始这些重心是随机的（也有一些更加有效的用于初始化重心的算法）<br>2. 寻找最近的重心并且更新聚类分配。将每个数据点都分配给这 K 个聚类中的一个。每个数据点都被分配给离它们最近的重心的聚类。这里的「接近程度」的度量是一个超参数——通常是欧几里得距离（Euclidean distance）。<br>3. 将重心移动到它们的聚类的中心。每个聚类的重心的新位置是通过计算该聚类中所有数据点的平均位置得到的。<br>重复2、3步，直到该算法收敛|
|聚类|层次聚类|不知道应该分为几类，使用层次聚类|1. 首先从 N 个聚类开始，每个数据点一个聚类。<br>2. 将彼此靠得最近的两个聚类融合为一个。现在你有 N-1 个聚类。<br>3. 重新计算这些聚类之间的距离。<br>4. 重复第 2 和 3 步，直到你得到包含 N 个数据点的一个聚类。<br>5. 选择一个聚类数量，然后在这个树状图中划一条水平线。

##### 4.2.2.2. 降维
|类别|算法|简介|方法|
|--|--|--|--|
|降维|主成分分析 – PCA|主成分分析是把多指标转化为少数几个综合指标|1. 第一步计算矩阵 X 的样本的协方差矩阵 S（此为不标准PCA，标准PCA计算相关系数矩阵C）<br>2. 第二步计算协方差矩阵S（或C）的特征向量 e1,e2,…,eN和特征值 , t = 1,2,…,N<br>3. 第三步投影数据到特征向量张成的空间之中。利用下面公式，其中BV值是原样本中对应维度的值。<br>$newBV_{i,p}=\sum^{n}_{k=1}e_iBV_{i,p}$
|降维|奇异值分解 – SVD|奇异值分解（Singular Value Decomposition）是线性代数中一种重要的矩阵分解，奇异值分解则是特征分解在任意矩阵上的推广。在信号处理、统计学等领域有重要应用。|
#### 4.2.3. 强化学习
强化学习就是指模仿一个人（或动物）的学习，设想感知->行为->奖励循环的非常自然的动物行为。

一个人或者一个动物首先会通过感知他或者她所处的状态来了解环境。在此基础上，他或者她会选择一个“动作”，将他或者她带到另一个“状态”。那么他或她将获得“奖励”，循环重复，直到他或她消失。这种学习方式（称为强化学习）与传统监督机器学习的曲线拟合方法有很大不同。尤其是，强化学习学习得非常快，因为每一个新的反馈（例如执行一个行动并获得奖励）都被立即发送到影响随后的决定。

强化学习也提供了预测和优化的平滑整合，因为它在采取不同的行动时保持当前状态的信念和可能的转换概率，然后做出决定哪些行动可以导致最佳结果。
### 4.3. 模型评价指标
模型评价指标：
   1. 精准率(precision)与召回率(recall)
   2. 准确率(accuracy)和错误率(errorrate)
   3. f1-score
   4. ROC曲线、AUC
## 5. 参考文献
https://www.cnblogs.com/charlotte77/p/5606926.html
