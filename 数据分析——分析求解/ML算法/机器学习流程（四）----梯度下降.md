# 梯度下降

梯度下降法(gradient descent)是最优化理论里面找最优解的一種方法，主要是希望用梯度下降法找到函数的局部最小值，因为梯度的方向是走向局部最大的方向，所以在梯度下降法中是往梯度的反方向走。
要算一个函数f(x)的梯度有一个前提，就是这个函数要是任意可微分函数，这也是深度学习为什么都要找可微分函数出來当激活函数(activation function)。
一维度的向量x的梯度，通常用f'(x)表示。
多维度的向量x的梯度，通常用∇f(x)表示。

梯度下降法主要分以下四步

1. 确定代价函数
2. 选择起始点
3. 计算梯度
4. 按学习率前进
5. 循环3、4步，直到得到最低点

## 预测函数与代价函数(cost function)

以一元线性回归为例，预测函数就是
$$y = wx$$
对应的代价函数就是均方误差：
$$
e_1 = (y_1-w*x_1)^2 = (w*x_1-y_1)^2 \\
e_1 = w^2*x^2_1-2(w*x_1*y_1)+y^2_1 \\
e_1 = w^2*x^2_1-w*2(x_1*y_1)+y^2_1 \\
$$
均方误差e
$$
e = \frac{1}{n}\sum^n_{i=1} e_i=\frac{1}{n}(\sum^n_{i=1} x^2_i)*w^2+(-2\sum^n_{i=1} x_i*y_i)*w+\sum^n_{i=1} y^2_i \\
$$
x_i与y_i都是已知数，用abc分别代替不同项的系数可得
$$
e=a*w^2+b*w+c
$$
最终的目的就是找一个w，使得e最小。

## 梯度(Gradient)计算

怎么找到使得e最小的w，就需要用到梯度下降  
$$
e=a*w^2+b*w+c
$$
例子中的代价函数是一个一元二次函数，可知是一个凹函数，像一个山谷，当w落在谷底，也就是取图像最低点时，e最小    

寻找最低点的步骤如下：

   
1. **确定下降方向（梯度）**
先随意选一个w，这时w通常在山腰上，需要向陡峭程度最大的方向走，就能最快到达最低点。这个最大的陡峭程度就是**梯度**，是代价函数的导数，本文例子就是当前w点的函数切线斜率。
2. **确定下降速度（学习率）**
   $w_新 = w_旧-梯度*学习率$
3. 循环1、2步，直到找到使得e最小的w



## 为什么要用梯度下降

因为现实生活中很多问题很复杂，有多个维度，代价函数也千变万化，超过三维后，已经无法可视化，这时用公式求数值解往往无法计算。而梯度下降法能适应各种情况。

## 目前梯度下降法的变体

BGD：最原始的梯度下降法，每次都计算所有样本点的损失，最精确，但计算代价高，速度慢。
SGD：每次只取一个样本点计算损失，速度快，但精度低
MBGD：每次取一批样本计算损失，是BGD与SGD的折中法

## 多维中梯度的理解
在空间的每一个点都可以确定无限多个方向，一个多元函数在某个点也必然有无限多个方向。因此，导数在这无限多个**方向导数中最大的一个**（它直接反映了函数在这个点的变化率的数量级）等于多少？它是**沿什么方向**达到的？描述这个最大方向导数及其所沿方向的矢量，就是我们所说的梯度。

概念：梯度是一个向量，梯度的方向就是函数在这点增长最快的方向，梯度的模就是该点的变化率，以此类推，降低最快的就是梯度的反方向，变化最慢的就和梯度垂直。

$$
grad f(x,y) = \nabla f(x,y) = (f_x,f_y)=\frac{\partial f}{\partial x}i+\frac{\partial f}{\partial y}j
$$