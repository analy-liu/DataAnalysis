 # 机器学习流程（二）----基础机器学习算法
本篇介绍各类基础机器学习算法的简介，以及算法的特点
算法实现或详细原理与运用即调参，见具体算法文档
- [机器学习流程（二）----基础机器学习算法](#机器学习流程二----基础机器学习算法)
  - [1. 监督学习](#1-监督学习)
    - [1.1. 回归（Regression）](#11-回归regression)
      - [1.1.1. 线性回归(Linear Regression)](#111-线性回归linear-regression)
      - [1.1.2. 多项式回归(Polynomial Regression)](#112-多项式回归polynomial-regression)
      - [1.1.3. 岭回归(Ridge Regression)](#113-岭回归ridge-regression)
      - [1.1.4. 套索回归 (Lasso Regression)](#114-套索回归-lasso-regression)
      - [1.1.5. 弹性网络回归(ElasticNet Regression)](#115-弹性网络回归elasticnet-regression)
      - [1.1.6. 逐步回归（stepwise regression）](#116-逐步回归stepwise-regression)
      - [1.1.7. 回归树](#117-回归树)
    - [1.2. 分类（Classification）](#12-分类classification)
      - [1.2.1. Logistic回归（LR）](#121-logistic回归lr)
      - [1.2.2. 线性判别分析算法（LDA）](#122-线性判别分析算法lda)
      - [1.2.3. 朴素贝叶斯](#123-朴素贝叶斯)
      - [1.2.4. 支持向量机SVM](#124-支持向量机svm)
      - [1.2.5. KNN](#125-knn)
      - [1.2.6. 决策树](#126-决策树)
      - [1.2.7. 随机森林](#127-随机森林)
      - [1.2.8. XGBoost](#128-xgboost)
  - [2. 非监督学习](#2-非监督学习)
    - [2.1. 聚类（Clustering）](#21-聚类clustering)
      - [2.1.1. k-means聚类](#211-k-means聚类)
      - [2.1.2. 层次聚类](#212-层次聚类)
    - [2.2. 降维（Dimensionality Reduction）](#22-降维dimensionality-reduction)
      - [2.2.1. 主成分分析法（PAC）](#221-主成分分析法pac)
      - [2.2.2. 奇异值分解（SVD）](#222-奇异值分解svd)
  - [3. 参考文献](#3-参考文献)

机器学习算法大致分为监督学习与非监督学习
## 1. 监督学习
监督学习是一种目的明确的训练方式，数据集是有标签的。
监督学习的流程：
1. 选择一个适合目标任务的机器学习模型
2. 把训练集给机器去学习（监督学习需要给数据打标签）
3. 训练得到出方法论
4. 在测试集上使用方法论

### 1.1. 回归（Regression）
回归是一种用于连续型数值变量预测和建模的监督学习算法。
回归算法有很多，这些算法的不同主要在于三方面：
1. 自变量的个数：一个or多个
2. 因变量的类型：连续or离散
3. 回归线的形状：线性or非线性

对于那些有创意的人，如果你觉得有必要使用上面这些参数的一个组合，你甚至可以创造出一个没有被使用过的回归模型。

常用的回归算法有：线性回归、多项式回归、岭回归、Lasso回归、弹性网络回归、逐步回归、回归树  
逻辑回归虽然也是叫回归，但是不是预测算法，是用于分类的，将在分类算法中介绍。

#### 1.1.1. 线性回归(Linear Regression)
**简介**
线性回归中，因变量是连续的，自变量只有一个，回归线的性质是线性的。
用一个方程表示，即：$y = a + bx + e$  
a表示截距，b表示斜率，e是误差项  

线性回归的拟合方法为**最小二乘法**  
最小二乘法：实际值到回归线的垂直偏差(残差)的平方和最小，即
$\min(\sum (y_i-\hat{y_i})^2)$  
损失函数：  $\min||X_w-y||^2$
也可以采用如下公式进行直接求解$\theta=(X^TX)^{-1}X^Ty$  
最小二乘法是一种无偏估计。对于一个适定问题，X通常是列满秩的。  
当X不是列满秩时，或者某些列之间的线性相关性比较大时， $X^TX$的行列式接近于0  

**特点**
1. 速度快
2. 可解释性好
3. 不适合非线性
4. 无法解决多重共线性
5. 对异常值很敏感
#### 1.1.2. 多项式回归(Polynomial Regression)
**简介**
线性回归适合于线性可分的数据，当我们处理非线性可分的数据时可以使用多项式回归。在这种回归中，我们是要找到一条曲线来拟合数据点，可以表示成下面的式子：
**特点**
1. 能够拟合非线性可分的数据，更加灵活的处理复杂的关系
2. 因为需要设置变量的指数，所以它是完全控制要素变量的建模
3. 需要一些数据的先验知识才能选择最佳指数
4. 如果指数选择不当容易出现过拟合
#### 1.1.3. 岭回归(Ridge Regression)
**简介**
岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，针对模型中存在的共线性关系的为变量增加一个小的平方偏差因子(也就是正则项)，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。  
损失函数：  $\min||X_w-y||^2+z||w||^2$
**特点**
1. 能处理共线性数据问题
2. 有L2正则，防止过拟合
3. 没有特征选择功能

#### 1.1.4. 套索回归 (Lasso Regression)
**简介**
Lesso与岭回归非常相似，都是在回归优化函数中增加了一个偏置项以减少共线性的影响，从而减少模型方程。不同的是Lasso回归中使用了绝对值偏差作为正则化项。  
损失函数：  $\min||X_w-y||^2+z||w||$
岭回归和Lasso回归之间的差异可以归结为L1正则和L2正则之间的差异
**特点**
1. 能处理共线性数据问题
2. 内置特征选择
3. 可解释性高
#### 1.1.5. 弹性网络回归(ElasticNet Regression)
**简介**
弹性回归网络是Lesso回归和岭回归技术的混合体。它使用了L1和L2正则化，也达到了两种技术共有的效果
损失函数：  $\min||X_w-y||^2+z||w||+z||w||^2$
**特点**
1. 鼓励在高度相关变量的情况下的群体效应，而不像Lasso那样将其中一些置为0.当多个特征和另一个特征相关的时候弹性网络非常有用。Lasso倾向于随机选择其中一个，而弹性网络倾向于选择两个。
2. 对所选变量的数量没有限制。

#### 1.1.6. 逐步回归（stepwise regression）
**简介**
逐步回归，是通过逐步将自变量输入模型，如果模型具统计学意义，并将其纳入在回归模型中。同时移出不具有统计学意义的变量。最终得到一个自动拟合的回归模型。其本质上还是线性回归。
**特点**
1. 自动化移除掉不显著的特征
2. 有可能移除掉重要特征

#### 1.1.7. 回归树
### 1.2. 分类（Classification）
常用的分类算法有：Logistic回归（LR）、线性判别分析算法（LDA）、朴素贝叶斯、支持向量机SVM、决策树、随机森林、XGBoost

#### 1.2.1. Logistic回归（LR）

#### 1.2.2. 线性判别分析算法（LDA）
#### 1.2.3. 朴素贝叶斯
#### 1.2.4. 支持向量机SVM
#### 1.2.5. KNN
#### 1.2.6. 决策树
#### 1.2.7. 随机森林
#### 1.2.8. XGBoost
## 2. 非监督学习
无监督学习本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。它具有三个特点：
1. **无监督学习没有明确目的的训练方式，你无法提前知道结果是什么。**
2. **无监督学习不需要给数据打标签。**
3. **无监督学习几乎无法量化效果如何。**
### 2.1. 聚类（Clustering）
常见的聚类算法有：K均值聚类、层次聚类

#### 2.1.1. k-means聚类
1. 简介
   K-means 是基于欧式距离的聚类算法，其认为两个目标的距离越近，相似度越大。
2. 算法步骤
   1. 选择k个样本作为初始聚类中心
   2. 对数据集中的每个样本，计算其到每个聚类中心的距离，并将其分到距离最小的聚类中心，最后将数据分为k类
   3. 对每个类别，重新计算聚类中心
   4. 重复2、3，直到某个中止条件（迭代次数、最小误差变化等）
3. 复杂度
   时间复杂度：$O(tknm)$，其中，t 为迭代次数，k 为簇的数目，n 为样本点数，m 为样本点维度。
   空间复杂度：$O(m(n+k))$，其中，k 为簇的数目，m 为样本点维度，n 为样本点数。
4. 优缺点
   优点：
   * 容易理解，聚类效果不错，虽然是局部最优， 但往往局部最优就够了；
   * 处理大数据集的时候，该算法可以保证较好的伸缩性；
   * 当簇近似高斯分布的时候，效果非常不错；
   * 算法复杂度低。

   缺点：
   * K 值需要人为设定，不同 K 值得到的结果不一样；
   * 对初始的簇中心敏感，不同选取方式会得到不同结果；
   * 对异常值敏感；
   * 样本只能归为一类，不适合多分类任务；
   * 不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。


#### 2.1.2. 层次聚类
### 2.2. 降维（Dimensionality Reduction）
常见的降维算法有：主成分分析法（PAC）、奇异值分解（SVD）

#### 2.2.1. 主成分分析法（PAC）
#### 2.2.2. 奇异值分解（SVD）
## 3. 参考文献
5 Types of Regression and their properties, George Seif, https://towardsdatascience.com/5-types-of-regression-and-their-properties-c5e1fa12d55e

