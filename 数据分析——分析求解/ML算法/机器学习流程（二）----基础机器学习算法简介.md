 # 机器学习流程（二）----基础机器学习算法简介
本篇介绍各类基础机器学习算法的简介，以及算法的特点
算法实现或详细原理与运用即调参，见具体算法文档
- [机器学习流程（二）----基础机器学习算法简介](#机器学习流程二----基础机器学习算法简介)
  - [1. 监督学习](#1-监督学习)
    - [1.1. 回归（Regression）](#11-回归regression)
      - [1.1.1. 线性回归(Linear Regression)](#111-线性回归linear-regression)
      - [1.1.2. 多项式回归(Polynomial Regression)](#112-多项式回归polynomial-regression)
      - [1.1.3. 岭回归(Ridge Regression)](#113-岭回归ridge-regression)
      - [1.1.4. 套索回归 (Lasso Regression)](#114-套索回归-lasso-regression)
      - [1.1.5. 弹性网络回归(ElasticNet Regression)](#115-弹性网络回归elasticnet-regression)
      - [1.1.6. 逐步回归（stepwise regression）](#116-逐步回归stepwise-regression)
      - [1.1.7. 回归树](#117-回归树)
    - [1.2. 分类（Classification）](#12-分类classification)
      - [1.2.1. Logistic回归（LR）](#121-logistic回归lr)
      - [1.2.2. 支持向量机SVM](#122-支持向量机svm)
      - [1.2.3. KNN](#123-knn)
      - [1.2.4. 决策树](#124-决策树)
      - [1.2.5. 随机森林](#125-随机森林)
  - [2. 非监督学习](#2-非监督学习)
    - [2.1. 聚类（Clustering）](#21-聚类clustering)
      - [2.1.1. k-means聚类](#211-k-means聚类)
      - [2.1.2. 层次聚类](#212-层次聚类)
    - [2.2. 降维（Dimensionality Reduction）](#22-降维dimensionality-reduction)
      - [2.2.1. 主成分分析法（PAC）](#221-主成分分析法pac)
  - [3. 参考文献](#3-参考文献)

机器学习算法大致分为监督学习与非监督学习
## 1. 监督学习
监督学习是一种目的明确的训练方式，数据集是有标签的。
监督学习的流程：
1. 选择一个适合目标任务的机器学习模型
2. 把训练集给机器去学习（监督学习需要给数据打标签）
3. 训练得到出方法论
4. 在测试集上使用方法论

### 1.1. 回归（Regression）
回归是一种用于连续型数值变量预测和建模的监督学习算法。
回归算法有很多，这些算法的不同主要在于三方面：
1. 自变量的个数：一个or多个
2. 因变量的类型：连续or离散
3. 回归线的形状：线性or非线性

对于那些有创意的人，如果你觉得有必要使用上面这些参数的一个组合，你甚至可以创造出一个没有被使用过的回归模型。

常用的回归算法有：线性回归、多项式回归、岭回归、Lasso回归、弹性网络回归、逐步回归、回归树  
逻辑回归虽然也是叫回归，但是不是预测算法，是用于分类的，将在分类算法中介绍。

#### 1.1.1. 线性回归(Linear Regression)
**简介**
线性回归中，因变量是连续的，自变量只有一个，回归线的性质是线性的。
用一个方程表示，即：$y = a + bx + e$  
a表示截距，b表示斜率，e是误差项  

线性回归的拟合方法为**最小二乘法**  
最小二乘法：实际值到回归线的垂直偏差(残差)的平方和最小，即
$\min(\sum (y_i-\hat{y_i})^2)$  
损失函数：  $\min||X_w-y||^2$
也可以采用如下公式进行直接求解$\theta=(X^TX)^{-1}X^Ty$  
最小二乘法是一种无偏估计。对于一个适定问题，X通常是列满秩的。  
当X不是列满秩时，或者某些列之间的线性相关性比较大时， $X^TX$的行列式接近于0  

**特点**
1. 速度快
2. 可解释性好
3. 不适合非线性
4. 无法解决多重共线性
5. 对异常值很敏感
#### 1.1.2. 多项式回归(Polynomial Regression)
**简介**
线性回归适合于线性可分的数据，当我们处理非线性可分的数据时可以使用多项式回归。在这种回归中，我们是要找到一条曲线来拟合数据点，可以表示成下面的式子：
**特点**
1. 能够拟合非线性可分的数据，更加灵活的处理复杂的关系
2. 因为需要设置变量的指数，所以它是完全控制要素变量的建模
3. 需要一些数据的先验知识才能选择最佳指数
4. 如果指数选择不当容易出现过拟合
#### 1.1.3. 岭回归(Ridge Regression)
**简介**
岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，针对模型中存在的共线性关系的为变量增加一个小的平方偏差因子(也就是正则项)，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。  
损失函数：  $\min||X_w-y||^2+z||w||^2$
**特点**
1. 能处理共线性数据问题
2. 有L2正则，防止过拟合
3. 没有特征选择功能

#### 1.1.4. 套索回归 (Lasso Regression)
**简介**
Lesso与岭回归非常相似，都是在回归优化函数中增加了一个偏置项以减少共线性的影响，从而减少模型方程。不同的是Lasso回归中使用了绝对值偏差作为正则化项。  
损失函数：  $\min||X_w-y||^2+z||w||$
岭回归和Lasso回归之间的差异可以归结为L1正则和L2正则之间的差异
**特点**
1. 能处理共线性数据问题
2. 内置特征选择
3. 可解释性高
#### 1.1.5. 弹性网络回归(ElasticNet Regression)
**简介**
弹性回归网络是Lesso回归和岭回归技术的混合体。它使用了L1和L2正则化，也达到了两种技术共有的效果
损失函数：  $\min||X_w-y||^2+z||w||+z||w||^2$
**特点**
1. 鼓励在高度相关变量的情况下的群体效应，而不像Lasso那样将其中一些置为0.当多个特征和另一个特征相关的时候弹性网络非常有用。Lasso倾向于随机选择其中一个，而弹性网络倾向于选择两个。
2. 对所选变量的数量没有限制。

#### 1.1.6. 逐步回归（stepwise regression）
**简介**
逐步回归，是通过逐步将自变量输入模型，如果模型具统计学意义，并将其纳入在回归模型中。同时移出不具有统计学意义的变量。最终得到一个自动拟合的回归模型。其本质上还是线性回归。
**特点**
1. 自动化移除掉不显著的特征
2. 有可能移除掉重要特征

#### 1.1.7. 回归树

### 1.2. 分类（Classification）
常用的分类算法有：Logistic回归（LR）、支持向量机SVM、决策树、随机森林

#### 1.2.1. Logistic回归（LR）

**简介**

Logistic函数是常用的二分类算法，假设因变量 y 服从伯努利分布。
它的拟合函数是有名的sigmond函数
$$f(z)=\frac{1}{1+e^{-x }}$$
sigmoid函数是一个s形曲线，取值范围在0-1之间

#### 1.2.2. 支持向量机SVM

**简介**
支持向量机（Support Vector Machine, SVM）也是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的“线性分类器”。其学习策略是找到一个分类的规则使分类的间隔最大化。间隔越大，说明我们分类的可信度就越高。

#### 1.2.3. KNN

**简介**

最邻近规则分类（K nearest neighbors，KNN）算法通过将不同的数据点分到不同的类中，来解决分类或回归问题。  
K是指最近的K个点，通常用欧式距离选取最近的点，  
当用于求解回归时，预测值就是最近K个点的平均值。

#### 1.2.4. 决策树

**简介**
决策树算法可以用于描述一个起因事件及其产生的结果。因为一个起因可以有多个结果，所以我们可以用树状结构来表示它们。

#### 1.2.5. 随机森林

**简介**

随机森林算法在决策树基础上更进一步，顾名思义，其用随机的方式构建一个“事件森林”，“森林”里面有很多的决策树组成。随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（作为分类算法的情况），然后看看哪一类被选择最多，就预测这个样本为那一类。

## 2. 非监督学习
无监督学习本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。它具有三个特点：
1. **无监督学习没有明确目的的训练方式，你无法提前知道结果是什么。**
2. **无监督学习不需要给数据打标签。**
3. **无监督学习几乎无法量化效果如何。**
### 2.1. 聚类（Clustering）
常见的聚类算法有：K均值聚类、层次聚类

#### 2.1.1. k-means聚类
1. 简介
   K-means 是基于欧式距离的聚类算法，其认为两个目标的距离越近，相似度越大。
2. 算法步骤
   1. 选择k个样本作为初始聚类中心
   2. 对数据集中的每个样本，计算其到每个聚类中心的距离，并将其分到距离最小的聚类中心，最后将数据分为k类
   3. 对每个类别，重新计算聚类中心
   4. 重复2、3，直到某个中止条件（迭代次数、最小误差变化等）
3. 复杂度
   时间复杂度：$O(tknm)$，其中，t 为迭代次数，k 为簇的数目，n 为样本点数，m 为样本点维度。
   空间复杂度：$O(m(n+k))$，其中，k 为簇的数目，m 为样本点维度，n 为样本点数。
4. 优缺点
   优点：
   * 容易理解，聚类效果不错，虽然是局部最优， 但往往局部最优就够了；
   * 处理大数据集的时候，该算法可以保证较好的伸缩性；
   * 当簇近似高斯分布的时候，效果非常不错；
   * 算法复杂度低。

   缺点：
   * K 值需要人为设定，不同 K 值得到的结果不一样；
   * 对初始的簇中心敏感，不同选取方式会得到不同结果；
   * 对异常值敏感；
   * 样本只能归为一类，不适合多分类任务；
   * 不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。


#### 2.1.2. 层次聚类

**简介**

层次聚类(Hierarchical Clustering)是聚类算法的一种，通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层，树的顶层是一个聚类的根节点。创建聚类树有自下而上合并和自上而下分裂两种方法。

层次聚类的合并算法通过计算两类数据点间的相似性，对所有数据点中最为相似的两个数据点进行组合，并反复迭代这一过程。简单的说层次聚类的合并算法是通过计算每一个类别的数据点与所有数据点之间的距离来确定它们之间的相似性，距离越小，相似度越高。并将距离最近的两个数据点或类别进行组合，生成聚类树。层次聚类使用欧式距离来计算不同类别数据点间的距离（相似度）。

### 2.2. 降维（Dimensionality Reduction）
常见的降维算法有：主成分分析法（PAC）

#### 2.2.1. 主成分分析法（PAC）

主成分分析（Principle component analysis）简称PCA，是常用的降维方法之一。通过将n维的数据集降维到n'低纬度空间；使得降维之后数据集尽可能的代表原数据集同时降维之后的损失尽可能的小。

- 算法流程​
  假定数据集包含m个n维数据。
  1. 将原始数据按照n行m列进行排列
  2. 在每一个维度上进行中心化，也就是每个数据减去同一行中的均值方便后面的计算
  3. 计算数据集的协方差矩阵
  4. 对协方差求取特征值与特征向量
  5. 将特征向量 按照其特征值大小从上至下排列成矩阵
  6. 根据认为设定的K值，将数据集映射到K维的低纬空间

## 3. 参考文献
5 Types of Regression and their properties, George Seif, https://towardsdatascience.com/5-types-of-regression-and-their-properties-c5e1fa12d55e

