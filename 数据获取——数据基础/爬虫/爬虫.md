 # python 爬虫
- [python 爬虫](#python-爬虫)
  - [1. python爬虫包的介绍与使用](#1-python爬虫包的介绍与使用)
    - [1.1. requests](#11-requests)
      - [1.1.1. requests.get](#111-requestsget)
        - [1.1.1.1. requests.get参数设置](#1111-requestsget参数设置)
        - [1.1.1.2. requests.get返回值](#1112-requestsget返回值)
      - [1.1.2. requests.post](#112-requestspost)
        - [1.1.2.1. requests.post参数设置](#1121-requestspost参数设置)
        - [1.1.2.2. requests.post返回值](#1122-requestspost返回值)
    - [1.2. lxml](#12-lxml)
      - [1.2.1. lxml.etree](#121-lxmletree)
        - [1.2.1.1. 使用xpath获取html文件目标值语法](#1211-使用xpath获取html文件目标值语法)
        - [1.2.1.2. 使用xpath获取html文件目标值例子](#1212-使用xpath获取html文件目标值例子)
    - [1.3. selenium](#13-selenium)
      - [基本操作](#基本操作)
      - [页面等待](#页面等待)
      - [警告框处理](#警告框处理)
      - [下拉框选择](#下拉框选择)
      - [cookie操作](#cookie操作)
      - [设置无界面](#设置无界面)
  - [2. 解析网页技巧](#2-解析网页技巧)
    - [2.1. 突破前端反调试--阻止页面不断debugger](#21-突破前端反调试--阻止页面不断debugger)
    - [2.2. 破解反爬虫技巧](#22-破解反爬虫技巧)
      - [2.2.1. Headers反爬虫](#221-headers反爬虫)
      - [2.2.2. IP限制](#222-ip限制)
      - [2.2.3. User-Agent限制](#223-user-agent限制)
    - [cookie解析](#cookie解析)
      - [cookie属性](#cookie属性)
  - [参考文献](#参考文献)
## 1. python爬虫包的介绍与使用
### 1.1. requests
requests是一个Python第三方库，用于访问网络资源，处理URL资源特别方便  
安装 ：

    pip install requests
#### 1.1.1. requests.get
##### 1.1.1.1. requests.get参数设置
```python
requests.get(url: str | bytes, params: SupportsItems | Tuple | Iterable | str | bytes | None = ..., **kwargs) -> Response

# 简单使用(只使用url参数)
target_url = 'https://github.com/'
r = requests.get(target_url)

# 进阶使用(使用params参数)
payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.get("'https://www.google.com.hk/search", params=payload)
# r.url的输出为：
# 'https://www.google.com.hk/search?key2=value2&key1=value1'

# 详细使用(使用**kwargs)
proxy = {
    'https': 'https://127.0.0.1:1080',
    'http': 'http://127.0.0.1:1080'
}
header = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.68'
## 设置头部信息，代理ip，不使用证书
r = requests.get(target_url, verify=False, headers=header, proxies=proxy) 
```
##### 1.1.1.2. requests.get返回值
返回值通常命名为 r 或 response
|代码|描述|
|:-|:-|
|**r.status_code**|响应状态码|
|**r.raw**|原始响应体，使用r.raw.read()读取|
|**r.content**|字节方式的响应体，需要进行解码|
|**r.content.decode**|("编码格式") 当编码格式与响应头部的字符编码相同时，内容与r.text|
|**r.text**|字符串方式的响应体，会自动更具响应头部的字符编码进行解码|
|**r.headers**|以字典对象储存服务器响应头，但是这个字典比较特殊，字典键不区分大小写，若键不存在，则返回None|
|**r.json()**|request中内置的json解码器|
|**r.raise_for_status()**|请求失败(非200响应)，抛出异常|
|**r.url**|获取请求的url|
|**r.cookies**|获取请求后的cookies|
|**r.encoding**|获取编码格式|

#### 1.1.2. requests.post
##### 1.1.2.1. requests.post参数设置
```
requests.post(url, data=None, json=None, **kwargs)
```
url: 必填  

data与json  根据请求头中的Content-Type情况选择一个输入  
1. Content-Type: application/x-www-form-urlencoded;
使用data，传入dict
2. Content-Type: application/json
使用json，传入str

```python
import json
# dict转json
json_data = json.dumps(dict_data)
# json转dict
dict_data = json.loads(json_data)
```
##### 1.1.2.2. requests.post返回值
同requests.get
### 1.2. lxml
lxml是一个Python库，使用它可以轻松处理XML和HTML文件，还可以用于web爬取。  
卸载：

    pip uninstall lxml  
安装：

    python -m pip install lxml -i https://pypi.tuna.tsinghua.edu.cn/simple
#### 1.2.1. lxml.etree
导入

    from lxml import etree
##### 1.2.1.1. 使用xpath获取html文件目标值语法
```python
# 根据html字符串建立html_xml, 这里使用requests.get的返回值
html_xml = etree.HTML(r.text)
target_element = html_xml.xpath('xpath')
```
##### 1.2.1.2. 使用xpath获取html文件目标值例子
案例html：
```html
<!DOCTYPE html>\n<html lang="zh-CN">
<head>
</head>
<body>
    <ul id="menu" class="menu">
        <li><a title="1" href="https://www.1.com">一</a></li>
        <li><a title="2" href="https://www.2.com">二</a></li>
        <li><a title="3" href="https://www.3.com">三</a></li>
    </ul>
    <ul id="text" class="text">
        <li><a title="1" href="https://www.text1.com">text一</a></li>
        <li><a title="2" href="https://www.text2.com">text二</a></li>
        <li><a title="3" href="https://www.text3.com">text三</a></li>
    </ul>
</body>
</html>
```
```python
#获取id="menu"的ul中的所有href
target_element = html_xml.xpath('\\ul[@id="menu"\li\a\@href')
# 返回
["https://www.1.com"，"https://www.2.com"，"https://www.3.com"]

#获取id="text"的ul中的倒数第二个文本内容
target_element = html_xml.xpath('\\ul[@id="text"\li[last()-1]\a\text()')
# 返回
["text二"]
```
### 1.3. selenium
准备工作：
1. 安装：pip install selenium
2. 下载浏览器驱动
  Firefox浏览器驱动：[geckodriver](https://github.com/mozilla/geckodriver/releases)
  Chrome浏览器驱动：[chromedriver](https://sites.google.com/a/chromium.org/chromedriver/home) 需要开全局代理、[taobao镜像](http://npm.taobao.org/mirrors/chromedriver/)
  IE浏览器驱动：[IEDriverServer](http://selenium-release.storage.googleapis.com/index.html)
  Edge浏览器驱动：[MicrosoftWebDriver](https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver)
  Opera浏览器驱动：[operadriver](https://github.com/operasoftware/operachromiumdriver/releases)
  PhantomJS浏览器驱动：[phantomjs](http://phantomjs.org/)
3. 设置环境变量
#### 基本操作
```python
from selenium import webdriver
# 打开浏览器
driver = webdriver.Edge(executable_path='MicrosoftWebDriver位置')

# 打开目标网页
target_url = ""
driver.get(target_url)

# 页面操作
driver.set_window_size(480, 800) # 页面大小
driver.back()# 后退 
driver.forward()# 前进 
driver.refresh() # 刷新
driver.save_screenshot("保存位置")# 页面截图

# 寻找元素
element = driver.find_element_by_xpath("")
# function
find_element_by_id()
find_element_by_name()
find_element_by_class_name()
find_element_by_tag_name()
find_element_by_link_text()
find_element_by_partial_link_text()
find_element_by_xpath()
find_element_by_css_selector()
find_elements_by...()

# 元素操作
element.click()# 点击元素
element.clear() # 清除文本
element.send_keys('')# 在元素中输入
element.screenshot('保存位置')# 元素截图
element.send_keys('').submit()# 输入并回车提交
element.send_keys('D:\\upload_file.txt')# 文件上传

# 元素判断
element.is_displayed()# 判断元素用户是否可见
element.is_enabled()# 元素是否可以点击
EC.presence_of_element_located((By.ID,"kw"))   #查看某个元素是否存在
EC.element_to_be_clickable()         #查看元素是否可点击
EC.element_located_to_be_selected((By.ID,"kw")) #某个预期元素是否被选中
element.get_attribute(name)# 获得属性值
element.text # 获取元素的文本
element.size # 获取元素的尺寸

# 键盘操作
send_keys(Keys.BACK_SPACE) 删除键（BackSpace）
send_keys(Keys.SPACE) 空格键(Space)
send_keys(Keys.TAB) 制表键(Tab)
send_keys(Keys.ESCAPE) 回退键（Esc）
send_keys(Keys.ENTER) 回车键（Enter）
send_keys(Keys.CONTROL,'a') 全选（Ctrl+A）
send_keys(Keys.CONTROL,'c') 复制（Ctrl+C）
send_keys(Keys.CONTROL,'x') 剪切（Ctrl+X）
send_keys(Keys.CONTROL,'v') 粘贴（Ctrl+V）
send_keys(Keys.F1) 键盘 F1
……
send_keys(Keys.F12) 键盘 F12

# 鼠标操作
# 引入 ActionChains 类
from selenium.webdriver.common.action_chains import ActionChains
element = driver.find_element_by_link_text("设置")# 设置元素
ActionChains(driver).move_to_element(element).perform()# 悬停操作
# function
perform()： 执行所有 ActionChains 中存储的行为；
context_click()： 右击；
double_click()： 双击；
drag_and_drop()： 拖动；
move_to_element()： 鼠标悬停。

# 关闭浏览器
driver.close()
```
#### 页面等待
```python
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
element = WebDriverWait(driver, 5, 0.5).until(
                      EC.presence_of_element_located((By.ID, "kw"))
                      )
```
**function**
WebDriverWait(driver, timeout, poll_frequency=0.5, ignored_exceptions=None)
driver ：浏览器驱动。
timeout ：最长超时时间，默认以秒为单位。
poll_frequency ：检测的间隔（步长）时间，默认为0.5S。
ignored_exceptions ：超时后的异常信息，默认情况下抛NoSuchElementException异常。

WebDriverWait()一般由until()或until_not()方法配合使用，下面是until()和until_not()方法的说明。
until(method, message=‘’) 调用该方法提供的驱动程序作为一个参数，直到返回值为True。
until_not(method, message=‘’) 调用该方法提供的驱动程序作为一个参数，直到返回值为False。
```python
WebDriverWait(driver,10).until(EC.title_is(u"百度一下，你就知道"))
# 判断title,返回布尔值

WebDriverWait(driver,10).until(EC.title_contains(u"百度一下"))
# 判断title，返回布尔值

WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID,'kw')))
# 判断某个元素是否被加到了dom树里，并不代表该元素一定可见，如果定位到就返回WebElement

WebDriverWait(driver,10).until(EC.visibility_of_element_located((By.ID,'su')))
# 判断某个元素是否被添加到了dom里并且可见，可见代表元素可显示且宽和高都大于0

WebDriverWait(driver,10).until(EC.visibility_of(driver.find_element(by=By.ID,value='kw')))
# 判断元素是否可见，如果可见就返回这个元素

WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'.mnav')))
# 判断是否至少有1个元素存在于dom树中，如果定位到就返回列表

WebDriverWait(driver,10).until(EC.visibility_of_any_elements_located((By.CSS_SELECTOR,'.mnav')))
# 判断是否至少有一个元素在页面中可见，如果定位到就返回列表

WebDriverWait(driver,10).until(EC.text_to_be_present_in_element((By.XPATH,"//*[@id='u1']/a[8]"),u'设置'))
# 判断指定的元素中是否包含了预期的字符串，返回布尔值

WebDriverWait(driver,10).until(EC.text_to_be_present_in_element_value((By.CSS_SELECTOR,'#su'),u'百度一下'))
# 判断指定元素的属性值中是否包含了预期的字符串，返回布尔值

#WebDriverWait(driver,10).until(EC.frame_to_be_available_and_switch_to_it(locator))
#  判断该frame是否可以switch进去，如果可以的话，返回True并且switch进去，否则返回False注意这里并没有一个frame可以切换进去

WebDriverWait(driver,10).until(EC.invisibility_of_element_located((By.CSS_SELECTOR,'#swfEveryCookieWrap')))
# 判断某个元素在是否存在于dom或不可见,如果可见返回False,不可见返回这个元素注意#swfEveryCookieWrap在此页面中是一个隐藏的元素

WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.XPATH,"//*[@id='u1']/a[8]"))).click()
# 判断某个元素中是否可见并且是enable的，代表可点击
driver.find_element_by_xpath("//*[@id='wrapper']/div[6]/a[1]").click()
WebDriverWait(driver,10).until(EC.element_to_be_clickable((By.XPATH,"//*[@id='wrapper']/div[6]/a[1]"))).click()

WebDriverWait(driver,10).until(EC.staleness_of(driver.find_element(By.ID,'su')))
# 等待某个元素从dom树中移除

WebDriverWait(driver,10).until(EC.element_to_be_selected(driver.find_element(By.XPATH,"//*[@id='nr']/option[1]")))
# 判断某个元素是否被选中了,一般用在下拉列表

WebDriverWait(driver,10).until(EC.element_selection_state_to_be(driver.find_element(By.XPATH,"//*[@id='nr']/option[1]"),True))
# 判断某个元素的选中状态是否符合预期

WebDriverWait(driver,10).until(EC.element_located_selection_state_to_be((By.XPATH,"//*[@id='nr']/option[1]"),True))
# 判断某个元素的选中状态是否符合预期
driver.find_element_by_xpath(".//*[@id='gxszButton']/a[1]").click()

instance = WebDriverWait(driver,10).until(EC.alert_is_present())
# 判断页面上是否存在alert,如果有就切换到alert并返回alert的内容
instance.accept()
# 关闭弹窗
```
#### 警告框处理
```python
alert = driver.switch_to_alert()
```
text：返回 alert/confirm/prompt 中的文字信息。
accept()：接受现有警告框。
dismiss()：解散现有警告框。
send_keys(keysToSend)：发送文本至警告框。keysToSend：将文本发送至警告框。
#### 下拉框选择
```python
from selenium import webdriver
from selenium.webdriver.support.select import Select
from time import sleep

driver = webdriver.Chrome()
driver.implicitly_wait(10)
driver.get('http://www.baidu.com')
sel = driver.find_element_by_xpath("//select[@id='nr']")
Select(sel).select_by_value('50')  # 显示50条
```
#### cookie操作
```python
#WebDriver操作cookie的方法：

driver.get_cookies() #获得所有cookie信息。
driver.get_cookie(name)# 返回字典的key为“name”的cookie信息。
driver.add_cookie(cookie_dict) # 添加cookie。“cookie_dict”指字典对象，必须有name 和value 值。
driver.delete_cookie(name,optionsString)# 删除cookie信息。“name”是要删除的driver.cookie的名称，“optionsString”是该cookie的选项，目前支持的选项包括“路径”，“域”。
driver.delete_all_cookies()# 删除所有cookie信息

# 使用cookie登录
for item in cookies:
    driver.add_cookie(item)
```
#### 设置无界面
```python
from selenium import webdriver # 模拟登录
from selenium.webdriver.chrome.options import Options
chrome_options = Options()
chrome_options.add_argument('--headless')
driver = webdriver.Chrome(executable_path=r"D:\chrome\chromedriver.exe", chrome_options=chrome_options)
```
## 2. 解析网页技巧
### 2.1. 突破前端反调试--阻止页面不断debugger
在debugger处添加条件断点，条件为false
参考网站：https://segmentfault.com/a/1190000012359015

### 2.2. 破解反爬虫技巧
#### 2.2.1. Headers反爬虫
   1. 检查: Cookie、Referer、User-Agent
   2. 解决方案: 通过F12获取headers,传给requests.get()方法
        
#### 2.2.2. IP限制
   1. 网站根据IP地址访问频率进行反爬,短时间内限制IP访问
   2. 解决方案: 
        a. 构造自己IP代理池,每次访问随机选择代理,经常更新代理池
        b. 购买开放代理或私密代理IP
        c. 降低爬取的速度
        
#### 2.2.3. User-Agent限制
   1. 类似于IP限制，检测频率
   2. 解决方案: 构造自己的User-Agent池,每次访问随机选择
        a. fake_useragent模块
        b. 新建py文件,存放大量User-Agent

### cookie解析
#### cookie属性
浏览器打开F12，在应用程序选项卡下面有Cookie选项。
可以看到一个Cookie有：Name、Value、Domain、Path、Expires/Max-Age、Size、HTTP、Secure这些属性。
1. Name
   Name表示Cookie的名称，服务器就是通过name属性来获取某个Cookie值。
2. Value
   Value表示Cookie 的值，大多数情况下服务器会把这个value当作一个key去缓存中查询保存的数据。
3. Domain
   Domain表示可以访问此cookie的域名。
   顶级域名只能设置或访问顶级域名的Cookie，二级及以下的域名只能访问或设置自身或者顶级域名的Cookie
4. Path
   Path表示可以访问此cookie的页面路径。比如path=/test，那么只有/test路径下的页面可以读取此cookie。
5. Expires/Max-Age
   Expires/Max-Age表示此cookie超时时间。若设置其值为一个时间，那么当到达此时间后，此cookie失效。不设置的话默认值是Session，意思是cookie会和session一起失效。当浏览器关闭(不是浏览器标签页，而是整个浏览器) 后，此cookie失效。
6. Size
   Size表示Cookie的name+value的字符数，比如有一个Cookie：id=666，那么Size=2+3=5 。
7. httponly
   若此属性为true，则只有在http请求头中会带有此cookie的信息，而不能通过document.cookie来访问此cookie。
8. Secure
   Secure表示是否只能通过https来传递此条cookie。不像其它选项，该选项只是一个标记并且没有其它的值。

## 参考文献
https://zhuanlan.zhihu.com/p/61536685,python之战

